## mpibind options

These are the options to control mpibind: 

```
-o mpibind=off|on
-o mpibind=verbose
-o mpibind=smt:<n>
-o mpibind=greedy:0|1
-o mpibind=gpu_optim:0|1
-o mpibind=corespec(first|numa|bal):<n>
-o mpibind=omp_places|omp_proc_bind|visible_devices 
```

When setting more than one option, use commas to separate them, e.g., `-o mpibind=smt:2,verbose`.

### Turn mpibind on or off

If mpibind is disabled by default, turn it on with `-o mpibind=on`

If mpibind is enabled by default, turn it off with `-o mpibind=off`

### Enable verbosity

To display the mapping of tasks to CPUs, use `-o mpibind=verbose`

### Specify an SMT level

To specify how many hardware threads per core to use for the application, use `-o mpibind=smt:<n>`, where `n` ranges between 1 and the number of hardware threads per core.

To use two hardware threads per core on an SMT-4 architecture, for instance, use `-o mpibind=smt:2`

By default mpibind uses one hardware thread per core. 

### Turn greedy on or off

To minimize remote memory accesses, mpibind nominally assigns one NUMA domain per task. When launching less tasks than NUMA domains, this can significantly limit the resources available to the application.

To use all of the resources of a node when using less tasks than NUMA domains, use `-o mpibind=greedy:1`

To assign a single NUMA domain to every task even when using less tasks than NUMA domains, use `-o mpibind=greedy:0`

By default greedy mode is on.

### Enable GPU optimized mappings

On some heterogeneous architectures the best mapping depends on the type of resource the application will use the most.

To fine-tune the mapping provided by mpibind for CPU usage, use `-o mpibind=gpu_optim:0`

To fine-tune the mapping provided by mpibind for GPU usage, use `-o mpibind=gpu_optim:1`

On systems with GPUs, GPU-optimized mapping is on.

### Turn on core specialization

On systems with significant noise generated by system processes, an application can avoid using certain cores with the expectation that the OS will use those for processing system tasks.

mpibind provides three core specialization schemes: `first`, `numa`, and `balanced`.

To avoid using the first `n` cores of every node, use `-o mpibind=corespecfirst:n`. This setting results in an imbalanced assignment of cores to tasks, since the first task is assigned less cores than its peers.

To avoid using `n` cores per NUMA domain, use `-o mpibind=corespecnuma:n`. This setting could result in an imbalanced assignment of cores to tasks if there are more than one task per NUMA domain. The total number of cores taken out per node is `n * nnumas`, where `nnumas` is the number of NUMA nodes per node.   

To avoid using `n` cores per task, use `-o mpibind=corespecbal:n`. This usually results in a balanced assigment of cores to tasks since every task avoids using the same number of cores. The toal number of cores take out per node is `n * ntasks`, where `ntasks` is the number of tasks per node.

By default core specialization is off.

### Disable OpenMP affinity or GPU affinity 

To enable OpenMP affinity and GPU affinity, mpibind sets certain environment variables. One can instruct mpibind not to set them as follows.

To not set OMP_PROC_BIND and OMP_PLACES, use `-o mpibind=omp_proc_bind` and `-o mpibind=omp_places`, respectively. To not set both, use `-o mpibind=omp_proc_bind,omp_places`

To not set GPU affinity, use `-o mpibind=visible_devices`. This setting applies to AMD and NVIDIA GPUs.

By default OpenMP affinity and GPU affinity are enabled.














