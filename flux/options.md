## mpibind options

These are the options and environment variables to control mpibind:

### Options

```
-o mpibind=off|on
-o mpibind=verbose
-o mpibind=smt:<n>
-o mpibind=greedy:0|1
-o mpibind=gpu_optim:0|1
-o mpibind=omp_places|omp_proc_bind|visible_devices 
```

When setting more than one option, use commas to separate them, e.g., `-o mpibind=smt:2,verbose`.

### Environment variables

```
MPIBIND_RESTRICT_TYPE=cpu|mem
MPIBIND_RESTRICT=<list-of-integers>
MPIBIND_TOPOFILE=<xml-file>
FLUX_MPIBIND_USE_TOPOFILE=<value>
```

---

### Turn mpibind on or off

If mpibind is disabled by default, turn it on with `-o mpibind=on`

If mpibind is enabled by default, turn it off with `-o mpibind=off`

### Enable verbosity

To display the mapping of tasks to CPUs and GPUs, use `-o mpibind=verbose`

### Specify an SMT level

To specify how many hardware threads per core to use for the application, use `-o mpibind=smt:<n>`, where `n` ranges between 1 and the number of hardware threads per core.

To use two hardware threads per core on an SMT-4 architecture, for instance, use `-o mpibind=smt:2`

By default mpibind uses one hardware thread per core. 

### Turn greedy on or off

To minimize remote memory accesses, mpibind nominally assigns one NUMA domain per task. When launching less tasks than NUMA domains, this can significantly limit the resources available to the application.

To use all of the resources of a node when using less tasks than NUMA domains, use `-o mpibind=greedy:1`

To assign a single NUMA domain to every task even when using less tasks than NUMA domains, use `-o mpibind=greedy:0`

By default greedy mode is on.

### Enable GPU optimized mappings

On some heterogeneous architectures the best mapping depends on the type of resource the application will use the most.

To fine-tune the mapping provided by mpibind for CPU usage, use `-o mpibind=gpu_optim:0`

To fine-tune the mapping provided by mpibind for GPU usage, use `-o mpibind=gpu_optim:1`

On systems with GPUs, GPU-optimized mapping is on by default.

### Enable core or thread specialization to mitigate system noise

On systems with significant noise generated by system processes, hardware resources can be dedicated for running these processes, e.g., system cores. On such systems user jobs should not be scheduled on these resources.

One can tell mpibind to schedule application work on a specific subset of the compute node to, for example, avoid using system resources.

One can specify the application resources in a memory-driven or compute-driven fashion: When MPIBIND_RESTRICT_TYPE is set to `cpu` one specifies a set of Linux CPUs and when this variable is set to `mem` one specifies a list of (NUMA) memory domains. When specifying a NUMA domain all of the compute resources local to that domain are included in the set. By default MPIBIND_RESTRICT_TYPE is set to `cpu`.

The MPIBIND_RESTRICT variable is then used to specify the IDs of the resources to use for application work.

For example, to restrict the application resources to the first and third NUMA domains (and their local resources) one would set `MPIBIND_RESTRICT_TYPE=mem` and `MPIBIND_RESTRICT=0,2`; and to restrict the application to CPUs 12-24 one would set `MPIBIND_RESTRICT_TYPE=cpu` and `MPIBIND_RESTRICT=12-24`.

On machines where these variables are set by default (presumably to mitigate system noise), one can unset these variables to regain access to the full node, but one has to be cognizant of the potential implications of running on the same resources as other system processes.


### Disable OpenMP affinity or GPU affinity 

To enable OpenMP affinity and GPU affinity, mpibind sets certain environment variables. One can instruct mpibind not to set them as follows.

To not set OMP_PROC_BIND and OMP_PLACES, use `-o mpibind=omp_proc_bind` and `-o mpibind=omp_places`, respectively. To not set both, use `-o mpibind=omp_proc_bind,omp_places`

To not set GPU affinity, use `-o mpibind=visible_devices`. This setting applies to AMD and NVIDIA GPUs.

By default OpenMP affinity and GPU affinity are enabled.

### Read in the machine topology

Discovering the node topology can be an expensive operation. When running under Flux, mpibind gets the topology specification from Flux rather than querying the topology once again.

Alternatively, one can tell mpibind to read the topology (1) from a static hwloc file or (2) dynamically. The former can be accomplished by setting `FLUX_MPIBIND_USE_TOPOFILE` to any non-empty value and `MPIBIND_TOPOFILE` to the hwloc-xml-file. The latter can be accomplished by setting `FLUX_MPIBIND_USE_TOPOFILE` only.
